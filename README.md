# Graph Fusion Experiment

> Explore operator fusion techniques in ONNX graphs to understand how graph optimizations affect inference performance.

## ğŸ¯ Purpose

Graph fusion is one of the most impactful optimizations for AI inference. This project demonstrates:
- **What fusion is**: Combining multiple operators into single optimized kernels
- **Why it matters**: Reduced memory bandwidth, fewer kernel launches
- **How to experiment**: Tools to manually fuse and measure impact

## ğŸ“Š Fusion Benefits

```
Before Fusion:              After Fusion:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MatMul â”‚                  â”‚                â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                  â”‚   Fused        â”‚
    â”‚        Memory         â”‚   MatMul+      â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”   Traffic        â”‚   Bias+        â”‚
â”‚  Add   â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â–¶      â”‚   ReLU         â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                  â”‚                â”‚
    â”‚                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”
â”‚  ReLU  â”‚                  Result:
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  - 1 kernel vs 3
                            - 2x less memory traffic
                            - 30-50% faster
```

## ğŸ“ Project Structure

```
Graph-Fusion-Experiment/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ graph_analyzer.py      # Analyze ONNX graphs
â”‚   â”œâ”€â”€ fusion_patterns.py     # Common fusion patterns
â”‚   â”œâ”€â”€ manual_fuser.py        # Apply manual fusions
â”‚   â””â”€â”€ benchmark.py           # Measure fusion impact
â”œâ”€â”€ patterns/
â”‚   â”œâ”€â”€ matmul_add.py          # MatMul + Add fusion
â”‚   â”œâ”€â”€ conv_bn_relu.py        # Conv + BatchNorm + ReLU
â”‚   â””â”€â”€ attention.py           # Multi-head attention fusion
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ fusion_analysis.ipynb
â””â”€â”€ models/
    â””â”€â”€ README.md              # Where to place test models
```

## ğŸš€ Quick Start

```python
from graph_fusion import GraphAnalyzer, ManualFuser

# Analyze fusion opportunities
analyzer = GraphAnalyzer("model.onnx")
opportunities = analyzer.find_fusion_opportunities()

print(f"Found {len(opportunities)} fusion opportunities:")
for opp in opportunities:
    print(f"  - {opp.pattern}: {opp.nodes}")

# Apply fusion
fuser = ManualFuser()
optimized = fuser.apply_pattern(
    "model.onnx",
    pattern="matmul_add",
    output="model_fused.onnx"
)

# Benchmark
from graph_fusion import benchmark_models
results = benchmark_models("model.onnx", "model_fused.onnx")
print(f"Speedup: {results['speedup']:.2f}x")
```

## ğŸ”§ Common Fusion Patterns

### 1. MatMul + Add (Bias)
```
MatMul(A, B) + C  â†’  Gemm(A, B, C)
```

### 2. Conv + BatchNorm + ReLU
```
Conv â†’ BatchNorm â†’ ReLU  â†’  ConvBnRelu (fused)
```

### 3. Attention Pattern
```
QKV Projection â†’ Reshape â†’ Attention â†’ Output
  â†’ FusedMultiHeadAttention
```

### 4. LayerNorm
```
ReduceMean â†’ Sub â†’ Pow â†’ ReduceMean â†’ Add â†’ Sqrt â†’ Div â†’ Mul â†’ Add
  â†’ LayerNormalization
```

## ğŸ“ˆ Expected Results

| Pattern | Unfused (ms) | Fused (ms) | Speedup |
|---------|--------------|------------|---------|
| MatMul+Add | 1.2 | 0.8 | 1.5x |
| Conv+BN+ReLU | 2.5 | 1.4 | 1.8x |
| Full Attention | 5.0 | 2.8 | 1.8x |
| Transformer Block | 15.0 | 8.0 | 1.9x |

## ğŸ“š Learning Resources

- [ONNX Graph Optimization](https://onnxruntime.ai/docs/performance/graph-optimizations.html)
- [Operator Fusion Paper (TVM)](https://arxiv.org/abs/1802.04799)
- [cuDNN Fusion Guide](https://docs.nvidia.com/deeplearning/cudnn/developer-guide/)

## License

MIT
